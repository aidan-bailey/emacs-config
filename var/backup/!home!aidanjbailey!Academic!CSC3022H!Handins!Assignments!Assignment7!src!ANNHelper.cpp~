#include "ANNHelper.hpp"

using namespace BLYAID001;
using namespace std;

ANNHelper::ANNHelper() {
  const int inputCount(3);
  const int hiddenCount(3);
  const int outputCount(1);
  PopulateGraph(inputCount, hiddenCount, outputCount);
}

void ANNHelper::PopulateGraph(int input_count, int hidden_count,
                              int output_count) {
  int layerCounts[3] = {input_count, hidden_count, output_count};
  int count(0);
  for (int layerIndex = 0; layerIndex < 3; layerIndex++) {
    vector<perceptron> perceptronLayer;
    for (auto nodeIndex = 0; nodeIndex < layerCounts[layerIndex]; nodeIndex++) {
      perceptron p{perceptron_type(layerIndex), count++, -1};
      perceptronLayer.push_back(p);
    }
    perceptron p{Bias, count++, 1};
    perceptronLayer.push_back(p);
    graph.push_back(perceptronLayer);
  }
}

void ANNHelper::BuildPaths() {
  for (int layerIndex = 0; layerIndex < int(graph.size() - 1); layerIndex++) {
    for (int nodeIndex = 0; nodeIndex < int(graph[layerIndex].size());
         nodeIndex++) {
      perceptron &current = graph[layerIndex][nodeIndex];
      for (auto targetPerceptron = graph[layerIndex + 1].begin();
           targetPerceptron != graph[layerIndex + 1].end();
           targetPerceptron++) {
        targetPerceptron->Connections.push_back(&current);
        targetPerceptron->Type == Bias ? current.Weights.push_back(1)
                                       : current.Connections.push_back(0);
      }
    }
  }
}

void ANNHelper::Train(int targetOutput, std::vector<float> inputList) {
  for (int i = 0; i < inputList.size(); i++)
    graph[0][i].State = inputList[i];

  for (int layerIndex = 1; layerIndex < int(graph.size() - 1); layerIndex++) {
    for (int nodeIndex = 0; nodeIndex < int(graph[layerIndex].size());
         nodeIndex++) {
      perceptron & current = graph[layerIndex][nodeIndex];
      float output = activationFunction(current);
    }
  }
}

float ANNHelper::perceptronRule(float oldWeight, float learningWeight,
                                float targetOutput, float currentOutput,
                                float input) {
  float result =
      oldWeight + learningWeight * (targetOutput - currentOutput) * input;
  return result;
}

float activationFunction(perceptron &node) {
  float sum(0);
  for (int perceptronIndex = 0; perceptronIndex < node.Connections.size();
       perceptronIndex++) {
    sum = sum + node.Connections[perceptronIndex]->State *
                    node.Weights[perceptronIndex];
  }
  return sum > 0 ? 1 : 0;
}
